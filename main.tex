\documentclass[11pt]{book}
\usepackage[a4paper, hmargin={4cm, 4cm}]{geometry}
\usepackage{eso-pic} % \AddToShipoutPicture
\usepackage{graphicx} % \includegraphics
\usepackage{minted}
\usepackage{listings}                 % Source code printer for LaTeX
\usepackage{caption}
\usepackage{parcolumns}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{multirow}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[RO, LE]{\thepage}
\fancyhead[LO]{\textsl{\leftmark}}
\fancyhead[RE]{\textsl{\rightmark}}

\renewcommand{\sectionmark}[1]{%
\markright{\thesection \ #1}{}}

\renewcommand{\chaptermark}[1]{%
\markboth{\thechapter \ #1}{}}

\titleformat{\chapter}[display]
  {\normalsize \huge  \color{black}}%
  {\flushleft\normalsize %
   \MakeUppercase{\fontfamily{}\fontsize{24}{24}\selectfont\chaptertitlename}\hspace{2ex}%
   {\fontfamily{}\fontsize{40}{40}\selectfont\thechapter}}%
  {10 pt}%
  {\hrule\vspace{10pt}\raggedleft\bfseries\huge}%


%% Change `ku-farve` to `nat-farve` to use SCIENCE's old colors or
%% `natbio-farve` to use SCIENCE's new colors and logo.
\def \ColourPDF {include/natbio-farve}

%% Change `ku-en` to `nat-en` to use the `Faculty of Science` header
\def \TitlePDF   {include/nat-en}  % University of Copenhagen

\title{
  \vspace{3cm}
  \Huge{A WebAssembly Backend for Futhark} \\
  \Large{Msc Thesis}
}

\author{
  \Large{Philip Lassen}
  \\ \texttt{philiplassen@gmail.com} \\
}

\date{
    \today
}



\begin{document}


\AddToShipoutPicture*{\put(0,0){\includegraphics*[viewport=0 0 700 600]{\ColourPDF}}}
\AddToShipoutPicture*{\put(0,602){\includegraphics*[viewport=0 600 700 1600]{\ColourPDF}}}

\AddToShipoutPicture*{\put(0,0){\includegraphics*{\TitlePDF}}}

\clearpage\maketitle
\thispagestyle{empty}

\newpage
\begin{center}
    \textbf{Abstract}
\end{center}

Futhark is a High-performance purely functional data-parallel array programming language targeting parallel compute hardware. Futhark has backends for several  compute architectures  and this thesis adds browsers because large percentage of toady's software is run in the browser, and the underlying hardware of the laptops and mobile phones that run these browser has increasingly parallel compute with GPU's and multicore CPU's. As such there are heavy development of standardized cross browser Web API's to harness these parallel architectures. One of these technologies is threaded WASM. This thesis develops a WASM and threaded WASM backend for the Futhark programming languages, leveraging both parallel computing and browsers as a target architecture.  




\tableofcontents



\chapter{Introduction}

\section{Thesis Structure}
\begin{itemize}
    \item Chapter 2, Background:
    Overview of the related work that this thesis builds on.
    \item Chapter 3, WASM:
    Explanation and analysis of WASM as a programming language and as a target language for the browser. 
    
    \item Chapter 4, WASM  Backend Implementation:
     Description of the implementation of the WASM backend. As well as a overview of the performance bench-marked against the native C backend.
    
    \item Chapter 5, Parallel Compute in the Browser:
    Analysis of the facilities and paradigms for parallel programming in the browser through JavaScript and WASM. 
    
    \item Chapter 6, WASM Multicore Backend Implementation:
    Description of the implementation of the multicore WASM backend. As well as a overview of the performance benchmarked against the native multicore C, and WASM backend developed in Chapter 4.
    
    \item Chapter 7, Conclusion:
    Summary of the implementations and performance of the backends developed. As well as a brief discussion of future developments for Futhark targeting parallel compute in the browser.
    
\end{itemize}

\chapter{Background}

TODO litter section with citations

%\section{Futhark}

TODO Futhark (C, POXIS threads backend)


%\section{Browsers}

The first web browsers started as Graphical User Interfaces for rendering static web pages. Web Browsers have evolved to keep up with the high paced innovation in web technologies and with modern day browsers having strong similarities with full blown operating systems. 


%\subsection{JavaScript}
JavaScript for a long period of time was the only supported programming language for the browser. As such it became one of the largest programming languages in the world. With browsers being one of the most ubiquitous coding platforms, programmers were restricted to JavaScript for any functionality in their websites that run client side. As a result many languages added backends to target JavaScript to that programmers could write code in their respective languages and generate JavaScript that would run in the browser. The problem is that JavaScript is not a particularly good target language, as it was not designed with this use case in mind. 

In the early days of web browsers, web pages would render differently across browsers as there was no standardization's for web API's. Different browsers supported different programming languages, creating big headaches for programmers, who wanted their websites to render identically across browsers.. One of the first popular languages for the browser was JavaScript. Eventually the big vendors converged on JavaScript releasing the standardized version ECMAScript. 

Huge investments have been made to increase the execution speed of JavaScript in the browser. With Google and Mozilla investing heavily into the V8 JavaScript engine and SpyderMonkey, (Add Safari and maybe Opera) respectively. Many approaches have been taken to make JavaScript faster. However they have all been fundamentally limited by the language design. 

%\subsection{ASM.js}

One of the approaches taken by the browser vendors was to define a subset of the language asm.js and a convention for type hints, which were designed for efficient execution by leveraging types and compiler tricks to allow ahead of time compilation. It was intended as a target language for compilation of statically typed programming languages. Emscripten was developed to be a C/C++ to asm.js compiler, utilizing the LLVM toolchain. 

%\subsection{WASM}
The major browser vendors collaboratively designed WASM to more comprehensively address the limitations of JavaScript as a target language for the web. It is a portable low level byte code, designed for compact representation, efficient compilation, and near native execution speeds. WASM is gaining adoption and has been used for a variety of applications (TODO: Google Earth and TensorFlow) especially as a target for compilation from C, C++ and Rust. The portability and execution speed have also led to adoption beyond browsers with use cases ranging from (TODO: DFinity and Edge computing). 

%\subsection{LLVM}
One of the technologies that has greatly helped the adoption of WASM as a target language is the LLVM compiler tool chain. Writing a full compiler from scratch that supports multiple targets is an incredibly time consuming process. In order to have high performance backends for different target architectures such x86 and ARM requires knowledge of many of the low level details of each respective target. An alternative approach is for the Compiler frontend of the source language to take the source code and translate it to the LLVM internal representation. As the LLVM compiler tool-chain can then generate high performance code on all the most common computer architectures. 


Many languages now support WASM as target language, either as a backend target from their main compiler or through another compiler implementation. Languages that generate LLVM IR have any easy path to WASM code generation. Many of the biggest languages are currently built with or have compiler implementations using LLVM.

While WASM has progressed the state of the art of single threaded computation speed in browsers an other avenue for execution speed is parallelism. Browsers have facilities for parallel programming. Javascript supports two different paradigms. Web  workers  and  message  passing  enable  parallel  programming  without  shared  memory. SharedArrayBuffer and atomics enable shared memory multithreading with thread synchronization.  There is a threaded WASM proposal that adds atomic operations to the language, and adds support for SharedArrayBuffers while relying on JavaScriptâ€™s web workers to create and join threads. 

TODO mention GPU API's for the browser

\chapter{WASM}
This chapter gives an overview of the design and structure of the WebAssembly programming language. This will be driven through an example program, and some performance evaluations and literature review. 



Its important to note that WebAssembly doesn't aim to be a complete replacement for JavaScript. WebAssembly cannot access the DOM for example. WebAssembly was instead designed to have good inter-operation with JavaScript. In practice a developer wanting to get high performance code running in the browser would first write their library in a language such as C or Rust. Then they would compile to WASM, and use JavaScript to call their WebAssembly module in the browser. 




WebAssembly aims to define a portable binary format that can be run in the web with high performance. A WebAssembly file is commonly referred to as a module, and given a \textit{.wasm} file extension. WebAssembly also defines a text format that serves to be a human readable version of the underlying binary format, much in the same way assembly provided a human readable format for machine code. 



\section{WebAssembly Module Structure}

WebAssembly files are segmented into sections. The segmentation of these sections is done so that loading a WASM file is efficient as possible. The sections are structured such that the byte code be compiled in a single pass, and in parallel. Furthermore the code can be parsed and compiled before the complete WASM file has been downloaded, reducing the instantiation time of a WebAssembly module. 
%\begin{table}[]
%\begin{tabular}{|l|l|}
%\hline
%ID & Section          \\ \hline
%0  & custom section   \\ \hline
%1  & typ section      \\ \hline
%2  & import section   \\ \hline
%3  & function section \\ \hline
%4  & table section    \\ \hline
%5  & memory section   \\ \hline
%6  & global section   \\ \hline
%7  & export section   \\ \hline
%8  & start section    \\ \hline
%9  & element section  \\ \hline
%10 & code section     \\ \hline
%11 & data section     \\ \hline
%12 & count section    \\ \hline
%\end{tabular}
%\end{table}

A WebAssembly module is split into different sections. These sections are split into two types of sections, known and custom sections. If a known section is included, it can only be included once, and most appear in a strict order. Where as custom sections can be included multiple times. Custom sections are just an arbitrary set of bytes that carry no meaning to WASM but can be useful for third party users. 

\begin{enumerate}
\addtocounter{enumi}{-1}
\item Custom section: 
\item 
\end{enumerate}


WebAssembly supports 4 number types.
\begin{verbatim}
i32: 32-bit integer
i64: 64-bit integer
f32: 32-bit float
f64: 64-bit float
\end{verbatim}
This has ramifications for inter operation with JavaScript, but is useful with respesct to supporting number types for languages like C/C++ and Rust that aims to be a target language for. 

A WebAssembly function has the following structure.
\begin{verbatim}
( func <signature> <locals> <body> )
\end{verbatim}
The signature gives the function name, parameter types, and return types. The locals are the local variables that will be used in the execution of the function, and the body is the actual implementation of the function. The most used WebAssembly example is the add function. 

\begin{verbatim}
(module
  (func (param $a i32) (param $b i32) (result i32)
    local.get $a
    local.get $b
    i32.add))
\end{verbatim}

The function signature of add specifies the two arguements a and b as i32s and specifies the return type result as i32. There are no locals. The funciton body has three instructions. The instructions \texttt{local.get \$a} and \texttt{local.get \$b} push the two arguments to onto the stack. The instruction \texttt{i32.add} pops the two elements off the stack and pushes their sum. The function returns the number on the stack. 

The body of the function could be replaced with 
\begin{verbatim}
 (i32.add (local.get $a) (local.get $b))
\end{verbatim}
that is, the WebAssembly allows the programmer to use a notation where arguments to instructions are passed as parameters instead of manually being placed on the stack.



%TODO link and ref mdn https://developer.mozilla.org/en-US/docs/WebAssembly/Understanding_the_text_format
\section{Memory}

A notion of memory is needed for writing more complex programs. In a language like C, it is common practice to use pointers to locations in memory, or for writing an array of values. Memory from the perspective of WASM is just an array of bytes that can be read from and written to. WebAssembly has two essential functions for interacting with this array, namely the \textit{load.i32} and \textit{store.i32}, for reading and writing to the array of bytes respectively.

As a motivating example the following C code gives a simple implementation of an place prefix sum.

\begin{verbatim}
void prefix_sum(int* arr, int size) {
  for (i = 1; i < size; i++) {
    arr[i] += arr[i-1];
  }
}
\end{verbatim}
The following code is a WebAssembly implementation of inplace prefix sum.
\begin{verbatim}
  1 (module
  2   (import "env" "memory" (memory $memory 1))
  3   (export "prefixSum" (func $prefixSum))
  4   (func $prefixSum (param $size i32)
  5     (local $offs i32)
  6     (local $acc i32)
  7     (local $last i32)
  8     (local.set $offs (i32.const 4))
  9     (local.set $last (i32.mul (local.get $size) (i32.const 4)))
 10     (local.set $acc (i32.load (i32.const 0)))
 11     loop $forloop
 12       (local.set $acc (i32.add (local.get $acc) (i32.load (local.get $offs))))
 13       (i32.store (local.get $offs) (local.get $acc))
 14       (local.set $offs (i32.add (local.get $offs) (i32.const 4)))
 15       (br_if $forloop (i32.ne (local.get $offs) (local.get $last)))
 16     end $forloop
 17   )
 18 )
\end{verbatim}
% TODO brief description of WASM
% TODO talk about 64k pages

For the implementation a local variable accumulator is set to the first value of the array, and an offset is set to 0. A loop is then entered, where the element at offset in the array is loaded with \texttt{i32.load} added to the accumulator and the result is stored with \texttt{i32.store}. The offset is increased by 4 bytes, and than compared to the local variable last. If it is not equal the loop goes back to the loop on line 12, and repeats.

The most important details of the function implementation for understanding WebAssembly's interaction with memory are the load and store operations, which use byte offsets to address memory. The memory is never explicitly referenced in the function because WebAssembly modules only have one declaration of memory in the memory section, making the array of memory implicit. Memory can either be imported from JavaScript, in which case the memory is created in JavaScript and passed to WebAssembly on instantiation, or alternatively the memory can be exported from WebAssembly, in which case the memory is created in WebAssembly on instantiation and can be accessed in JavaScript afterwards. In line 2 the memory is imported. Where the 
\begin{verbatim}
(import "env" "memory" (memory $memory 1))
\end{verbatim}

the ending 1 sets the size of the memory heap to be 1 page of memory. For WebAssembly 1 page corresponds to 64 kilobytes. Memory is always set to an integral number of pages.


\section{WASM and JavaScript interop}

WebAssembly defines a relatively small set of instructions, which runtimes can translate to native code quickly. However because of this it is unable to complete many of the basic functions of JavaScript. The most clear cut example of this is not having access to the DOM. 
WebAssembly does not have direct access to Web APIs. However it can call imported JavaScript functions. This is the facility that allows it to interact with Web APIs. This simple design feature is incredibly powerful. It is this feature that enables threaded WebAssembly as it can leverage JavaScript's ability to spawn web workers. This will be discussed in greater detail in Chapter 6.

\section{Performance}

The performance of WebAssembly

\section{WASM as a library}

For most practical use cases WebAssembly modules are used as libraries. When compiling C to WASM as a library the Emscripten compiler generates two files. One is a WebAssembly module and the other is JavaScript glue code. This allows web programmers to access all the important elements of WASM from a set of JavaScript functions that the Emscripten runtime defines. This is beneficial for multiple reasons. Firstly working directly with WebAssembly modules is a layer of detail most web developers would like to avoid. Secondly this approach allows a programmer to simply compile their C library and call it from JavaScript. 

\section{WebAssembly in the browser}

With the development of WebAssembly


\section{WASM outside the browser}

There are also a number of applications for WebAssembly outside of the browser. The most standard engine for running WebAssembly outside the browser is Node. Node also uses the V8 engine that is present in Chrome, and other browsers to support WebAssembly. As a result it is a high degree of performance parity with WebAssembly that is run in the browser.

Running Node locally is convenient for rapid prototyping as well as testing. Making tests that run in the browser takes a deal of configuration, and isn't trivial. It is possible to run Chrome, and Firefox as headless browsers for testing purposes, but often it is desirable to test part of WebAssembly functionality


WebAssembly executes within a sand-boxed stack-based environment, which means that running foreign code can only effect the virtual environment that executes the WASM module. This compounded with its competitive performance to native code makes it an ideal candidate for any applications that need to run foreign code. 




\chapter{WASM backend Implementation}

TODO make sure that in previous chapter we explictly explain why WebAssembly is better for JavaScript. 



This chapter describes the implementation of an additional Futhark backend that generates WebAssembly and JavaScript glue code such that Futhark programs can be compiled to libraries that can be run in the browser. The implementation includes an API to invoke the compiled libraries from JavaScript. The implementation is benchmarked in the Chrome browser, and in Node.js and against the C backend.

TODO maybe add some motivation

Firstly we design the JavaScript API. We take inspiration from the Futhark C and Python APIs. Our design factors in the characteristics and limitations of JavScript and WebAssembly. The API is illustrated with an example.


\begin{listing}[H] 
        \inputminted[fontsize=\small,baselinestretch=0.5,linenos]{C}{code/compiler/int128_c.c}
        \caption{64 bit multiplication with 128 bit casting}
        \label{lst:int128}    
\end{listing} 


Secondly we implement the backend. We take outset in the existing futhark Sequential C backend and pass the generated C code to the Emscripten compiler as the final compiler pass. The Emscripten generates both a WebAssembly module along with JavaScript glue code. We generate additional JavaScript code to wrap our API around the JavaScript code generated by Emscripten.

To illustrate the API and backend implementation working in practice we use the WebAssembly backend to efficiently generate graphics for the mandelbrot set in a webpage.

Finally we benchmark the Futhark backend to quantify its performance. The benchmarks come from the futhark benchmark suite\footnote{https://github.com/diku-dk/futhark-benchmarks}, which are standardized benchmarks to test the performance of Futhark backends against industry standards. The benchmarks show the WebAssembly, both when run in the Browser and run on Node locally, performs between 12\% to 65\% slower than the generated C running locally. 

\section{JavaScript API Design}

This section describes the JavaScript API, taking inspiration from Futhark's C and Python APIs. In practice WebAssembly modules aren't called directly instead programmer expect to call it from JavaScript. This allows them to aviod the low level details of working with WASM modules. 


It's insightful to look at the APIs of other languages so we copy the elements of the APIs that are effective and avoid the parts that are cumbersome or inefficient. These decisions are constrained by the limitations and capabilities of the target language, which for us is JavaScript and WebAssembly.

The exposition will be example based. To best illustrate the API designs we choose an example with multidimensional array inputs and outputs as well as a scalar input. 
\begin{verbatim}
entry scale (scalar : f32) (matrix : [][]f32) = map (map (scalar *)) matrix
\end{verbatim}
This program multiplies each element of a matrix by a scalar.

\subsection{C API}
When scale.fut is compiled as a C library, the files scale.c and scale.h are generated, being the implementation and header files. Listing TODO is a C program that interfaces with the generated library. 



The structure of the C API is that one first configures futhark and creates a futhark context. This creates a context and a mutex to guarantee thread safety if used in a multithreaded environment.  

Futhark has backends for both the C and Python programming languages. 
\section{Runtime}
In order to simplify the usage of the generated JavasScript and WASM a runtime is introduced. More specifically two classes are introduced. One of the classes FutharkValue, wraps Futhark values. The FutharkValue class has properties and methods for getting the dimension, and getting the underlying JavaScript array that corresponds to its data. The other class that we include in our runtime system wraps the futhark context, and provides the library functions for going to futhark types, and calling futhark functions. This way the API doesn't expose pointers to the user in the API. 


\subsection{FutharkContext}

For listing \ref{lst:futhark-capi} the JavaScript class FutharkContext defines the following functions.

\begin{listing}[H] 
        \inputminted[fontsize=\small,baselinestretch=0.5,linenos]{javascript}{code/compiler/c-backend/libnames.js}
        \caption{Futhark Context class and functions}
        \label{lst:jsapi}    
\end{listing} 
The constructor takes care of initializing the context. The two to functions allow the user to convert Javascript arrays into 1 dimensional or 2 dimensional futhark arrays. The from functions provide facilities from going from the futhark arrays to javascript types. In general the programmer may choose to use the from functions, but will more likely elect to simply call equivalent functions on the actual FutharkValue type, to get the actual values. Note that the Futhark Context both keeps track of the Futhark Context and Futhark Config, thus shielding the underlying pointers from the programmer.


\subsection{FutharkValue}

The FutharkValue class provides a helpful mean of encapsulating the futhark values, which can be
bool, i8, i16, i32, i64, u8, u16, u32, u64, f32, f64, or a multidimensional array of any of these types. JavaScript does not have a good notion of multiple number types. However JavaScript and most web browsers provide TypedArray classes, which can handle arrays of all futhark values, except Booleans. The mapping of the futhark types to their respective JavaScript TypedArray class can be seen in Listing \ref{lst:type-mapping}.

\begin{listing}[H]
\begin{minted}{JavaScript}
{'  i8' : Int8Array ,
 ' i16' : Int16Array ,
 ' i32' : Int32Array ,
 ' i64' : BigInt64Array ,
 '  u8' : Uint8Array ,
 ' u16' :  Uint16Array ,
 ' u32' :  Uint32Array ,
 ' u64' :  BigUint64Array ,
 ' f32' : Float32Array ,
 ' f64' : Float64Array ,
 'bool' : Uint8Array};
\end{minted}
\caption{Futhark type to JavaScript TypedArrays mapping}
\label{lst:type-mapping}
\end{listing}

In the case where a FutharkValue is a scalar, the underlying data is stored as a singleton TypedArray, so as to keep the encoding, it also keeps track of a string representing its type. This is a little redundant as the typed array mapping gives the type, but it is neccesary to have a mechanism to determine whether the Uint8Array is a uint8 or a bool, and the overhead of keeping track of a four letter character is minimal. The FutharkValue class also keeps track of the shape of the futhark value. The typed array can possibly be a multidimensional array, in which case the underlying array and a shape are sufficient to represent the data.

The other important aspect of the FutharkValue class is how it uses laziness to avoid unnecessary data copying. When an entry point function returns a FutharkValue, the FutharkValue stores a pointer, which is the location of the Futhark array on the Emscripten Heap (If the return type is a scalar than there are no pointers involved). If the user calls another futhark entry point without ever using this intermediary value, it would be inefficient to copy the data from the Emscripten Heap. Instead it makes most sense to only copy the data when the user explicitly asks for it. If the FutharkValue is just past into another entry point the implementation will simply use the underlying pointer. Both the shape and underlying data for the array are stored on the memory heap. For the user to extract the values, or shape they call the values() or shape() function on FutharkValue respectively.
\begin{listing}[H]
\begin{minted}{JavaScript}
      values() {
        if (!this.values_init) {
          this.values_ = this.lookup('values');
          this.values_init = true;
        }
        return this.values_;
      }
\end{minted}
\caption{FutharkValues values() method implementation}
\label{lst:values-implementation}
\end{listing}

The implementation of the values() function can be seen in listing \ref{lst:values-implementation}. A user who wants the underlying array representing the futhark data makes a call to values(). In which case if values hasn't already been copied from the Emscripten Heap, it calls the lookup function to get the array copied from the heap and than returns it. Otherwise this operation has already been called in the past, in which case its already been evaluated and the array can be returned without any additional work. 

%This chapter discusses the implementation of the initial WASM backend. It discusses the design choices made with considerations to code maintenance, performance, and practical usability. 

\section{WebAssembly Code Generation}

The most used WASM backends, namely Rust and C/C++ use LLVM to generate WASM code. The reason they are able to do this successfully is because the languages already have compiler frontends that go from the source language to the LLVM IR. With respect to the Futhark compiler, instead of emitting a standard compiler intermediate representation, it elects to generate C code that can in turn be compiled with any standard C compiler. This has the advantage of portability in the sense that it doesn't depend on LLVM which doesn't come standard on all Windows, and Linux distributions. Emscripten provides the facilities for going from C source code to WASM. The alternative to generating LLVM IR for it to produce WASM and using the Emscripten compiler on the generated C source code, is to generate the WASM by hand. These three choices will be discussed briefly.

\begin{itemize}
    \item Generating WASM directly: 
        WASM by hand is by far the least attractive of all the options. Writing low level code is both incredibly time consuming it is also inefficient, in the sense that it would be nearly impossible to capture enough of the  
    \item LLVM IR: This is the most standard approach among compilers. However since Futhark doesn't already generate LLVM IR utilizing this approach would effectively require a redesign of the compiler architecture. This would be an interesting project, with possible positive implications in more places than just a WASM backend. However this would be quite a large task, as the Futhark frontend wasn't designed with this specific intermediate representation in mind. 
    \item Emscripten: Emscripten can simply take C source code generated from the Futhark compiler. This approach would require the least modifications to the compiler. Futhark also aims to emit high performance C code, and Emscripten aims to translate C to high performance WASM code. Connecting the two compiler's together is the approach that is most logical given the current tooling of the futhark compiler.
\end{itemize}

Conveniently the Emscripten compiler is a full fledged production ready C and C++ to WASM compiler, used by the likes of Google, and Mozilla. In order to test the viability of taking generated C source code and passing it through the Emscripten compiler, some exploratory testing was necessary.

The simplest Futhark program takes no arguments and simply returns a constant. 
\begin{listing}[H]
\begin{minted}{Haskell}
let main = 1i32
\end{minted}
\caption{Futhark program returning 1}
\label{lst:simp}
\end{listing}



However when this code compiles to C. it generates over 2200 lines of code. This is because it contains logic for option parsing, and a fair deal of boiler plate code. Before going farther with the Emscripten compiler it is a worth while experiment to see what modifications have to be made to the generated C code before it can be accepted by the Emscripten compiler. One issue was in platform specific code used for timing, however on further inspection it turned out that this code was a relic and not actually used anywhere in the compiler, and therefor could just be deleted. Listing \ref{lst:int128} gives the other function implementation that Emscripten can't compile.
\begin{listing}[H] 
        \inputminted[fontsize=\small,baselinestretch=0.5,linenos]{C}{code/compiler/int128_c.c}
        \caption{64 bit multiplication with 128 bit casting}
        \label{lst:int128}    
\end{listing} 

The issue is that Emscripten and WASM don't support \textit{uint128} types. Instead an alternatate implementation for calculating the high order bits of 64 bit multiplication, that doesn't cast to \textit{uint128} numbers is used. 

\begin{listing}[H] 
        \inputminted[fontsize=\small,baselinestretch=0.5,linenos]{C}{code/compiler/int128_wasm.c}
        \caption{64 bit multiplication without 128 bit casting}
        \label{lst:integrate-js}    
\end{listing} 


With the small modifications described to get the generated C to a state where it can be compiled with the Emscripten compiler, Futhark programs, can be compile to WASM and run with Node as Executables. Note that this is specifically for executables and not C libraries. However it is of most interest to get Futhark running in the browser as a library such that web developers can design high performance libraries in futhark and then call them from JavaScript in their frontend. 


\subsection{JavaScript and WebAssembly number types}

In order use the JavaScript and WebAssembly code that is generated by Emscripten as a library, the C library functions need to be exported during the Emscripten compilation process. The library functions that need to be exported are the ones that the Futhark C backends spit out into the C header file, when the futhark program is compiled as a library. Again this whole process works almost out of the box when connecting Futhark's C library code generator with the appropriate Emscripten command. However an issue is that int64 types create cryptic errors when they are provided argument types to functions. 

JavaScript are 64-bit floating-point values. This means that all 32 bit numbers can be represented in JavaScript, but not all 64 bit numbers can be represented with full precision. WebAssembly fully support for 64-bit integers. This creates an issue as JavaScript can not call WebAssembly functions with appropriate type. This can be handled by instead passing two arguments to JavaScript functions, one for the low order 32 bits, and another for the high order 32 bits. This method is quite clunky, which is why Emscripten introduced the WASM\_BIGINT flag during compilation. It also come with a speed up over the old workaround. 
% Possibly show function taking two arguements here as an example
After introducing this compiler flag and with the appropriate functions exported from the header file,  %Reference https://v8.dev/features/wasm-bigint
a programmer with familiar with the semantics of the Emscripten heap has a workable WASM library. This involves keeping track of the pointers on the Emscripten heap, by passing them into the appropriate functions. However working with pointers in JavaScript is both not idiomatic and not safe. For Futhark to be practical the implementation details of the raw pointers to the heap should be extracted away, and should be shielded by a layer of abstraction.
Below is a simple futhark program for calculating the sum of rows in a matrix. 

\begin{listing}[H] 
        \inputminted[fontsize=\small,baselinestretch=0.5,linenos]{haskell}{code/compiler/c-backend/main.fut}
        \caption{Simple futhark row sum program}
        \label{lst:futhark-capi}   

\end{listing} 

Though this program is very simple. It accepts a 2d matrix of i32 numbers, and returns an array of i32. However the machinery for working with this in C is a little bit clunky. The code below shows how this program would be called from C.

\begin{listing}[H] 
        \inputminted[fontsize=\small,baselinestretch=0.5,linenos]{C}{code/compiler/c-backend/example.c}
        \caption{C API Usage for Simple futhark program}

        \label{lst:capi}    
\end{listing} 

Firstly a Futhark config and context must be initialized. The second important observation is that any array or matrix type must first be converted to an equivalent Futhark type before it can be used as an argument to one of Futhark's entry point functions. Similarly, any array or matrix type returned by Futhark must be converted back into C types by using Futhark library functions, if these values need to be used outside of a futhark context. C does not have an easy to use standard representation of multi-dimensional arrays. JavaScript also doesn't have a good notion of multidimensional arrays, in comparison to a lanuage like Python, where it can leverage the numpy ecosystem. 

\section{Runtime}
In order to simplify the usage of the generated JavasScript and WASM a runtime is introduced. More specifically two classes are introduced. One of the classes FutharkValue, wraps Futhark values. The FutharkValue class has properties and methods for getting the dimension, and getting the underlying JavaScript array that corresponds to its data. The other class that we include in our runtime system wraps the futhark context, and provides the library functions for going to futhark types, and calling futhark functions. This way the API doesn't expose pointers to the user in the API. 


\subsection{FutharkContext}

For listing \ref{lst:futhark-capi} the JavaScript class FutharkContext defines the following functions.

\begin{listing}[H] 
        \inputminted[fontsize=\small,baselinestretch=0.5,linenos]{javascript}{code/compiler/c-backend/libnames.js}
        \caption{Futhark Context class and functions}
        \label{lst:jsapi}    
\end{listing} 
The constructor takes care of initializing the context. The two to functions allow the user to convert Javascript arrays into 1 dimensional or 2 dimensional futhark arrays. The from functions provide facilities from going from the futhark arrays to javascript types. In general the programmer may choose to use the from functions, but will more likely elect to simply call equivalent functions on the actual FutharkValue type, to get the actual values. Note that the Futhark Context both keeps track of the Futhark Context and Futhark Config, thus shielding the underlying pointers from the programmer.


\subsection{FutharkValue}

The FutharkValue class provides a helpful mean of encapsulating the futhark values, which can be
bool, i8, i16, i32, i64, u8, u16, u32, u64, f32, f64, or a multidimensional array of any of these types. JavaScript does not have a good notion of multiple number types. However JavaScript and most web browsers provide TypedArray classes, which can handle arrays of all futhark values, except Booleans. The mapping of the futhark types to their respective JavaScript TypedArray class can be seen in Listing \ref{lst:type-mapping}.

\begin{listing}[H]
\begin{minted}{JavaScript}
{'  i8' : Int8Array ,
 ' i16' : Int16Array ,
 ' i32' : Int32Array ,
 ' i64' : BigInt64Array ,
 '  u8' : Uint8Array ,
 ' u16' :  Uint16Array ,
 ' u32' :  Uint32Array ,
 ' u64' :  BigUint64Array ,
 ' f32' : Float32Array ,
 ' f64' : Float64Array ,
 'bool' : Uint8Array};
\end{minted}
\caption{Futhark type to JavaScript TypedArrays mapping}
\label{lst:type-mapping}
\end{listing}

In the case where a FutharkValue is a scalar, the underlying data is stored as a singleton TypedArray, so as to keep the encoding, it also keeps track of a string representing its type. This is a little redundant as the typed array mapping gives the type, but it is neccesary to have a mechanism to determine whether the Uint8Array is a uint8 or a bool, and the overhead of keeping track of a four letter character is minimal. The FutharkValue class also keeps track of the shape of the futhark value. The typed array can possibly be a multidimensional array, in which case the underlying array and a shape are sufficient to represent the data.

The other important aspect of the FutharkValue class is how it uses laziness to avoid unnecessary data copying. When an entry point function returns a FutharkValue, the FutharkValue stores a pointer, which is the location of the Futhark array on the Emscripten Heap (If the return type is a scalar than there are no pointers involved). If the user calls another futhark entry point without ever using this intermediary value, it would be inefficient to copy the data from the Emscripten Heap. Instead it makes most sense to only copy the data when the user explicitly asks for it. If the FutharkValue is just past into another entry point the implementation will simply use the underlying pointer. Both the shape and underlying data for the array are stored on the memory heap. For the user to extract the values, or shape they call the values() or shape() function on FutharkValue respectively.
\begin{listing}[H]
\begin{minted}{JavaScript}
      values() {
        if (!this.values_init) {
          this.values_ = this.lookup('values');
          this.values_init = true;
        }
        return this.values_;
      }
\end{minted}
\caption{FutharkValues values() method implementation}
\label{lst:values-implementation}
\end{listing}

The implementation of the values() function can be seen in listing \ref{lst:values-implementation}. A user who wants the underlying array representing the futhark data makes a call to values(). In which case if values hasn't already been copied from the Emscripten Heap, it calls the lookup function to get the array copied from the heap and than returns it. Otherwise this operation has already been called in the past, in which case its already been evaluated and the array can be returned without any additional work. 

\section{Compiler Pipeline}

When putting all these pieces together and adding the modifications in the compiler an important implementation observation is that the API only depends on 3 things. The name of Futhark's C library functions, these functions return types and these functions arguments types. This greatly simplifies the implementation. The function names, return types, and arguement types are needed for generating the runtime classes, and exporting the functions to the emcc command. Firstly Futhark's C api is a part of the compiler that is most stable. Only depending on this part of the futhark compiler to generate JavaScript code means this section will rarely have to be adapted to deal with code changes in the intermediate representation of the compiler.

When Emscripten compiles C code it generates two files. One is a WASM module, which contains the instructions for computing the functions described in the C source code. The other file is JavaScript glue code, which is used to take care of loading the WASM file, as well as handling any JavaScript calls that the WASM module might use. In order to have a WASM module that could be easily used as a library without exposing the low level details of C, the FutharkValue and FutharkContext classes were introduced. Emscripten provides facilities for combining the JavaScript glue code with a library at a command line with the --post-fix and --js-library flags. This is convenient as it reduces the number of files that are produced from running the \textit{futhark wasm} command.



Figure \ref{fig:wasm} illustrates the logical flow of the compilation of the WASM backend. 

\begin{figure}[htbp]
\centerline{\includegraphics[width=\textwidth]{figures/WASM_MC_compiler.png}}
\caption{WASM-multicore compilation}
\label{fig:wasm}
\end{figure}

The source program is turned into Futhark's intermediate representation, and than into C source code from there. Where as the runtime classes that the WASM backend generates only depend only depend on a the function names, argument types, and return types, which can easily be taken from the IR. At this stage both the C source code and the API are joined together with the EMCC command, generating glue code with the attached runtime code, and a WASM module. 

\subsection{Emscripten Compiler Flags}
The Emscripten compiler provides a large number of flags that effect the usage and the performance of the generated WASM code. Some of the flags are specific to the use case, in which case it makes sense to give the programmer flexibiliy in terms of what compiler flags they elect to use. For this reason the flags the backend implementation elects to only use few flags. As discussed previously in order for 64-bit integers to be handled correctly, the backend needs to use the WASM\_BIGINT flag. The other flag that it defaults to is the O3 optimization level. Though this obfuscates the resulting code, it comes with sizeable performance improvements over the other optimization levels.

The most important flag for users of the WASM backend is the memory flag. The memory flag in Emscripten defaults to 16 megabytes, but with performance heavy computation it is likely that this limit will be exceeded. In these cases the user should manually set the memory based on their needs. It is recommended to use less than 2 gigabytes as only recently have virtual machines started to allow more than 2 gigabytes of heap space. This means that using more memory than 2gb's will likely not be portable across browsers and with different WASM engines. 

\begin{listing}
\begin{minted}{bash}
EMCFLAGS="-s MEMORY=((16777216 * 4))" futhark wasm --lib prog.fut
\end{minted}
\caption{Example futhark wasm compile command}
\label{lst:fu-wasm-compile}
\end{listing}

Listing \ref{lst:fu-wasm-compile} shows an example of calling the backend with an additional compiler flag to set the memory limit higher. The additional compiler flags to Emscripten are passed through the environment variable EMCFLAGS. In this case the memory is increased to 64 megabytes.
\section{Application}

With the WASM backend implemented as described above, it can now be seen in action. One of the applications for high performance computing in the browser is graphics. Figure \ref{fig:mandelbrot} illustrates a Futhark program running in the browser for visualizing the mandelbrot set. First mandelbrot.fut file is compiled as a lib. Than a web server is launched. When the HTML in (TODO cite appendix) is run the webpage in figure \ref{fig:mandelbrot} is rendered.



\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{figures/mandelbrot.png}
    \caption{Caption}
    \label{fig:mandelbrot}
\end{figure}

The only Futhark specific code required in to get the values to render is given below. The code is easy to call, and clearly hides the underlying pointers that are being passed around from the programmer.

\begin{minted}{javascript}
var fc = new FutharkContext();
var result = fc.main(screenX, screenY, depth, xmin, ymin, xmax, ymax);
var vals = result[0].values();
\end{minted}

\section{Benchmarking}

The WASM backend is benchmarked against the C backend to see how competitive the execution speed is. The WASM backend is both benchmarked running in the browser with Chrome as well as running with Node. 
\begin{itemize}
    \item Chrome: benchmarking WASM in Chrome gives details on how WASM performs in the browser, which is likely where the backend will be deployed in practice.
    \item Node: benchmarking with Node gives details into how the WASM preforms when run as a backend language. This is interesting as it gives details into the WASM's performance for use cases outside of the browser.
\end{itemize}



The Futhark programs that are benchmarked come from the Futhark benchmark suite, which are used for standardized benchmarks to test the performance of all Futhark backends against industry standards. What is interesting to look at is how the WASM performs relative to the C backend it is built on top of.
\begin{table}[h!]
    \noindent\makebox[\textwidth]{%
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Suite      & Dataset                     & Size & C        & WASM     & chrome \\ \hline
    \multirow{3}{*}{Accelerate} & \multirow{3}{*}{Tunnel} & 1000 & 735 ms  & 1,219 ms  & 1217  \\ \cline{3-6} 
               &                             & 2000 & 2,942 ms & 4,889 ms & 4,827 ms    \\ \cline{3-6} 
               &                             & 4000 & 11,762 ms & 19,693 ms & 19,302 ms   \\ \hline
    \end{tabular}%
    }
    \caption{Caption}
    \label{tab:my_label}
\end{table}

The Tunnel Benchmark shows that the WASM backend takes approximately 65\% performance penalty for running the benchmarks with WASM relative to the sequential C backend. This performance penalty is consistent as the dataset increases, which means that it is not a constant overhead of launching or instantiating WASM. WASM when run in Chrome and in Node have nearly identical performance, with the difference never exceeding 2 percentage points.

\begin{table}[h!]
    \noindent\makebox[\textwidth]{%
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    Suite      & Dataset                     & Size & C        & WASM     & chrome \\ \hline
    \multirow{3}{*}{Accelerate} & \multirow{3}{*}{Mandelbrot} & 1000 & 131 ms   & 149 ms  & TODO ms  \\ \cline{3-6} 
               &                             & 2000 & 530 ms  & 602 ms  & TODO ms    \\ \cline{3-6} 
               &                             & 4000 & 2,117 ms & 2,403 ms & TODO ms   \\ \hline
    \end{tabular}%
    }
    \caption{Caption}
    \label{tab:my_label}
\end{table}

For the mandelbrot benchmarks the WASM backend has more competitive execution speeds. The WASM backend is consistently 13-14\% slower across all the sizes. Similarly to the tunnel backend the relative performance difference of the C backend and the WASM does not change with respect to the dataset sizes.


TODO add more benchmarks

TODO add more discussion

TOOO add conclusion

\chapter{Parallel Execution in the Browser}


Browsers have facilities for parallel programming. Javascript supports two different paradigms. Web workers and message passing enable parallel programming without shared memory. SharedArrayBuffer and atomics enable shared memory multithreading with thread synchronization. There is a threaded WASM proposal that adds atomic operations to the language, and adds support for SharedArrayBuffers while relying on JavaScript's web workers to create and join threads. This chapter introduces all these concepts and illustrates them with examples.

JavaScript is single threaded. Meaning it consists of a single call stack and a single memory heap. This is slightly counter intuitive as idiomatic JavaScript often contains many asynchronous function calls. Asynchronous function calls are achieved by placing promises/callbacks into an event queue, which runs after the main thread has finished processing. This way they avoid blocking synchronous JavaScript code from running. 

%There are three primitives used for doing multithreaded programming in the browser: Web Workers, Shared Memory, and Atomics. These features will be introduced in this section along with examples illustrating how they are used in practice. 


\section{Web Workers}
Parallelism with JavaScript in browsers is achieved through web workers. Web workers are extra threads of execution beyond the main thread. The threads interact via message passing. Typically messages are passed through the postMessage and onmessage. postMessage is used to send a message between threads and onmessage works as an event handler to receive messages from threads. 


Web workers are relatively heavyweight, and should not be created in large numbers. They are expected to be long lived and have both high start and high per instance memory cost. 
TODO show why/where this information comes from (FOLLOW UP FROM ABOVE)

The following example computes the Riemann integral of sine over an interval from 0. The interval is broken up into subintervals which are computed by separate workers.

\begin{listing}[H] 
        \inputminted[fontsize=\small,baselinestretch=0.5,linenos]{javascript}{code/worker/integrate.js}
        \caption{Main file that calls workers which handle the computation of Riemann integral} 
        
        \label{lst:integrate-js}    
\end{listing} 

The example code in Listing \ref{lst:integrate-js} spawns 4 worker threads in lines 5-7. It sends each thread a message with their respective index in lines 18-20. It asynchronously waits for messages from each of the worker threads with their partial result and prints the final result when all the threads have sent a message in lines 10-16.
\begin{listing}[H] 
        \inputminted[fontsize=\small,baselinestretch=0.5,linenos]{javascript}{code/worker/worker.js}
        \caption{Worker thread logic for computing Riemann integral} 
        \label{lst:worker-js}    
\end{listing}    

The code in Listing \ref{lst:worker-js} contains the implementation of the worker threads. Once the thread receives a message from the main thread with their index, they compute the partial Riemann integral over their respective quartile of the interval adding the value of sine(x) as many times as specified by granularity. Figure \ref{png:integrate} shows the execution time of the code against a different number of web workers.



\begin{figure}[H]
\centerline{\includegraphics[width=\textwidth]{figures/integrate_exe_times.png}}
\caption{Execution time of Riemann integration for different thread counts. Run on a Macbook Pro with 2,2 GHz 6-Core Intel Core i7}
\label{png:integrate}
\end{figure}

The code execution time in \ref{png:integrate} is for the Reimann integral computed with one billion sample values. For one worker thread the execution time was 11.11 seconds. For two threads the execution time was 6.8 seconds, which is nearly double as fast. However as the number of threads increases the increase in execution speed tapers off. Going from 8 to 12 cores only yields a marginal increase in execution speed going from 2.45 second to 2.39. Once more than twelve worker threads are exceeded, there is no longer a marginal increase in speedup. This can be attributed to the physical limitation of threads on the hardware the code was executed on. The number of logical cores on the computer that executed this code was 12, meaning that any additional web worker launched after the initial 12 must wait on the thread pool for another to finish before it can be activated. In which case the overhead of launching it is only detrimental to the complete execution time of the program


\section{Shared Memory and Atomics}

Web workers with message passing have some similarities in how parallelism is executed with the Erlang programming language. Both of which use message passing to coordinate parallel execution. However many other programming languages and libraries also support and utilize shared memory. An example of this is C/C++ and POSIX threads. Shared memory maps closely to modern multicore hardware, and is faster for certain workloads (TODO GIVE EXAMPLE). However it comes at the cost of a new set of bugs in the shape of data races, which is why languages such as Erlang and Futhark itself abstracts the construct away from the programmer.

JavaScript also offers shared memory through SharedArrayBuffers. A SharedArrayBuffer points to a piece of linear memory. The SharedArrayBuffer can be passed to multiple web workers who can access the memory in parallel. 

In principle safe access to shared memory can be coordinated with message passing, but it's far more efficient for fine grained synchronization to use atomic operations, which again map efficiently to the underlying hardware. Atomic operations make sure that predictable values are written and read, that operations are finished before the next operation starts and that operations are not interrupted (TODO cite Atomics Javascript page). The Atomics package in JavaScript contains functions for performing atomic operations on SharedArrayBuffers. The Atomics package also includes wait and notify functions, like linux futex wait and wake.

To illustrate shared memory and atomics the following example is an implementation of prefix sum. It is a fundamental parallel algorithm which is used as a building block for many other parallel algorithms. The example implements the shared memory 2 pass algorithm from (TODO). 
\begin{listing}[H]    
        \inputminted[fontsize=\small,baselinestretch=0.5,linenos]{javascript}{code/shared/main.js}
        \caption{Main file that calls workers which compute prefix sum using shared memory and atomics in parallel}    
        \label{lst:main-js}    
\end{listing}    
The code in Listing \ref{lst:main-js} spawns 9 worker threads. It sends a message to each of the threads with parameters, some of which are shared array buffers. This allows each of the threads to have access to shared memory. When each thread has sent a message to indicate completion, the final result of prefix sum is logged to the console.

%\captionof{listing}{Example of a worker working}
\begin{listing}[H]    
\inputminted[fontsize=\small,baselinestretch=0.5,linenos]{javascript}{code/shared/prefix_sum.js}
        \caption{Worker file for computing the prefix sum using shared memory and atomics.}    
        \label{lst:prefixsum-js}    
\end{listing}    


The code in Listing \ref{lst:prefixsum-js} handles the actual execution of prefix sum. In the first pass of the algorithm, each thread calculates the prefix sum of their partition in the array, by calling the function prefix\_sum\_partition. Each thread signals that they are done with their work in the first pass by using the Atomics add, and notify functions. The first thread detects signal has accumulated a response of num\_workers - 1, by using the Atomics function wait. At this point the the first thread calculates the cumulative sums of partitions. At this stage it notifies the other other threads using the Atomics store and notify. At this stage all threads calculate the final prefix\_sum, using the precomputed values. On completion each thread sends a message back to the the main file using the postMessage to indicate they are done. 


%TODO: Show speed up over one worker vs 9 workers

%TODO update code to be cleaner and easier to explain

\section{Threaded WebAssembly}
There is a proposal to extend the WASM specification with support for threads, namely by leveraging web workers, shared memory, and atomics. Chrome and Firefox and Node.js all have experimental support for threaded WASM. Emscripten supports compilation of C/C++ with pthreads to threaded WASM.

Threaded WASM uses web workers to create and join threads. It doesn't natively invoke web workers but instead handles this by calling out to JavaScript. Shared memory is accomplished by integrating SharedArrayBuffer with WASM's paged memory model. WASM is extended with atomic operation instructions. Putting it concisely the additions of supporting shared array buffers in WASM and adding atomic operations in WASM was all that was needed to facilitate threaded WASM.

A key observation is that WASM does not natively allow for spawning of threads. This is actually taken care of by the runtime or compiler. Specifically for Emscripten, compiling C code written with pthreads will generate three files. It will generate a WASM file, and and two Javascript files. One for the main glue code and other for worker glue code. The glue code takes care of loading the WASM module, populating the memory with the required values, and integrating with the host system as the C code would expect. The C function pthread\_create is translated to Javascript and not WASM. It launches a Javascript Worker, passing it a shared array buffer and the wasm module that it should run. The WASM simply needs the shared array buffer and atomics to synchronize.  

\begin{figure}[htb]
\centerline{\includegraphics[width=\textwidth]{figures/UML_threaded_wasm.png}}
\caption{UML diagram showing flow of execution of threaded programs in WASM}
\label{uml}
\end{figure}

%TODO make updates to UML as specified on piece of paper.

The UML diagram in figure \ref{uml} demonstrates the execution flow of a parallel function written with pthreads in C, compiled by Emscripten, and run in Javascript and WASM. The javascript glue code calls the parallel WASM function, which then calls an external Javascript function that is used to emulate the functionality of pthread\_create. This function launches a new worker, sending it a message with the WASM module, and a shared array buffer, and resumes execution. This JavaScript worker code then  instantiates the WASM module, calling the designated WASM module function. And then with these multiple WASM modules running in parallel, they use atomic instructions native to WASM to facilitate synchronization, as specified in the program.

%TODO: Pthreads + memory growth talk about this

%TODO: Talk about specifying number of web workers up front with Emscripten

%TODO Explain why threaded WASM has no drawbacks becuase it uses the same facilites as javascript for parralel programming while having the speed up of WASM over JS. (Possibly look at over head of launching/loading WASM in a JS worker file).

\newpage

\chapter{WASM Multicore Backend}
The benchmarks in this section compares Tensorflow and Futhark performance on the
two networks used in the accuracy test, but the benchmark only include the training part of the program

This chapter details the extensions that are added to the Futhark compiler to support a multicore WASM backend. It also benchmarks the generated WASM-multicore code against the sequential WASM backend. It also compares the multicore C backend against the WASM-multicore backend running in the browser.

Fortunately only small adaptions had to be made to the WASM backend developed earlier, to get it running with Multicore. The futhark Compiler has a backend that generates both Sequential C code as well as a backend that generates multicore C code using POSIX threads. As discussed Emscripten can translate multicore C code that uses POSIX threads to multicore WASM that can run in parallel in the browser. The JavaScript API developed in chapter 4 stays unchanged for the WASM-multicore backend. 

Though this thesis adds 2 backends to the 6 backends already present in the Futhark compiler (4 C backends, and 2 Python backends), the added complexity is relatively modest because of the high degree of reuse of code between the two WASM backends. 


\section{Implementation Structure}

Below we discuss how to add a new futhark backend that can be invoked from the command line with \texttt{futhark wasm-multicore}. It is structured very similarly to the plain wasm backend described chapter 2. Instead of calling Emscripten on the Sequential C, we apply it to futhark's multicore C backend. We utilize the wasm runtime code written in chapter 2, for the backend and add the necessary Emscripten compiler flags required to enable Multicore WASM. Figure (TBD) illustrates the structure of the WASM multicore implementation.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\textwidth]{figures/WASM_MC_compiler.png}}
    \caption{WASM-multicore compilation}
\label{fig:wasm-mc}
\end{figure}
One of the key differences that can be seen in the figure \ref{fig:wasm-mc} is that the wasm-multicore backend produces 2 JavaScript files and 1 WASM file as opposed to the Sequential Wasm backend which generates 1 JavaScript file and 1 WASM file. The second JavaScript file is the web worker glue code.

\section{Implementation Details}
Here we discuss the series of steps that were needed complete the implementation.
\subsection{Actions}

The function \textit{compileMulticoreToWASMAction} contains the plumbing for going from futhark source code to WASM and JavaScript glue code. First it calls \textit{MulticoreWASM.compileProg}. This function take the Futhark source code, and runs the multicore C compiler on the source code, as well as generating the Javascript API code.

\textit{MulticoreWASM.compileProg} simply combines the multicore C compiler with the JavaScript code generation discussed in chapter 4, by calling the respective functions that contain the meat of the logic, which have already been developed. The last thing of note is that the flag \textit{-pthread} is passed to \textit{runEMCC}. This flag lets Emcc know that our C code contains pthreads, and thereby compiles it correctly.


\begin{listing}[H]    
        \inputminted[fontsize=\small,baselinestretch=0.5,linenos,breaklines]{haskell}{code/actionmc.hs}
        \caption{Main file that calls workers which compute prefix sum using shared memory and atomics in parallel}    
        \label{lst:action-mc}    
\end{listing}    
\subsection{Multicore C code changes}
The pthread C code generated by the multicore C backend used platform specific implementations in the generated function \textit{getrusage\_thread} and \textit{num\_processors}. For \textit{getrusage\_thread} it was possible to reuse the linux implementation. For \textit{num\_processors} it was necessary to add an additional implementation for the Emscripten platform. This was simply done by including \textit{emscripten/threading.h} and calling the function \textit{emscripten\_num\_logical\_cores}.




\subsection{Emscripten Invocation}

The generated multicore C code uses the POSIX function \textit{pthread\_create}. Emscripten aims to follow the POSIX standard closely, but in some places has slightly different behaviour. This is the case for \textit{pthread\_create}, which is a function that is used in the generated multicore C code. 

When pthread\_create() is called, if we need to create a new Web Worker, then that requires returning the main event loop. That is, you cannot call pthread\_create and then keep running code synchronously that expects the worker to start running - it will only run after you return to the event loop (TODO cite pthread docs). In order to work around the API differences, the compiler flag \textit{PTHREAD\_POOL\_SIZE=<integer} needs to be passed to the Emscripten compiler. This effectively creates the web workers before the main thread is called, in which case create\_pthread can just use an already spawned web worker. From testing, this parameter is best set to the number of logical\_cores of the underlying hardware.



\subsection{Running in browser with HTTP}



\section{Benchmark}






\chapter{Conclusion}

\end{document}
